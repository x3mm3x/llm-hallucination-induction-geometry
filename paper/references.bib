% ══════════════════════════════════════════════════════════════
% paper_2_references.bib — From Prerequisites to Predictions
% ══════════════════════════════════════════════════════════════

% ──────────────────────────────────────────────────────────────
% HALLUCINATION SURVEYS AND DETECTION
% ──────────────────────────────────────────────────────────────

@article{ji2023survey,
  title={Survey of Hallucination in Natural Language Generation},
  author={Ji, Ziwei and Lee, Nayeon and Frieske, Rita and Yu, Tiezheng and Su, Dan and Xu, Yan and Ishii, Etsuko and Bang, Ye Jin and Madotto, Andrea and Fung, Pascale},
  journal={ACM Computing Surveys},
  volume={55},
  number={12},
  pages={1--38},
  year={2023},
  publisher={ACM}
}

@article{zhang2023siren,
  title={Siren's Song in the {AI} Ocean: A Survey on Hallucination in Large Language Models},
  author={Zhang, Yue and Li, Yafu and Cui, Leyang and Cai, Deng and Liu, Lemao and Fu, Tingchen and Huang, Xinting and Zhao, Enbo and Zhang, Yu and Chen, Yulong and others},
  journal={arXiv preprint arXiv:2309.01219},
  year={2023}
}

@article{tonmoy2024comprehensive,
  title={A Comprehensive Survey of Hallucination Mitigation Techniques in Large Language Models},
  author={Tonmoy, S.M Towhidul Islam and Zaman, S M Mehedi and Jain, Vinija and Rani, Anku and Rawte, Vipula and Chadha, Aman and Das, Amitava},
  journal={arXiv preprint arXiv:2401.01313},
  year={2024}
}

@inproceedings{manakul2023selfcheckgpt,
  title={{SelfCheckGPT}: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models},
  author={Manakul, Potsawee and Liusie, Adian and Gales, Mark J.F.},
  booktitle={Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
  pages={9004--9017},
  year={2023}
}

@article{varshney2023stitch,
  title={A Stitch in Time Saves Nine: Detecting and Mitigating Hallucinations of {LLMs} by Validating Low-Confidence Generation},
  author={Varshney, Neeraj and Yao, Wenlin and Zhang, Hongming and Chen, Jianshu and Yu, Dong},
  journal={arXiv preprint arXiv:2307.03987},
  year={2023}
}

% ──────────────────────────────────────────────────────────────
% INTERNAL REPRESENTATIONS AND PROBING
% ──────────────────────────────────────────────────────────────

@inproceedings{burns2023discovering,
  title={Discovering Latent Knowledge in Language Models Without Supervision},
  author={Burns, Collin and Ye, Haotian and Klein, Dan and Steinhardt, Jacob},
  booktitle={International Conference on Learning Representations},
  year={2023}
}

@inproceedings{azaria2023internal,
  title={The Internal State of an {LLM} Knows When It's Lying},
  author={Azaria, Amos and Mitchell, Tom},
  booktitle={Findings of the Association for Computational Linguistics: EMNLP 2023},
  pages={967--976},
  year={2023}
}

@article{marks2024geometry,
  title={The Geometry of Truth: Emergent Linear Structure in Large Language Model Representations of True/False Datasets},
  author={Marks, Samuel and Tegmark, Max},
  journal={arXiv preprint arXiv:2310.06824},
  year={2024}
}

@article{li2024inference,
  title={Inference-Time Intervention: Eliciting Truthful Answers from a Language Model},
  author={Li, Kenneth and Patel, Oam and Vi{\'e}gas, Fernanda and Pfister, Hanspeter and Wattenberg, Martin},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

% ──────────────────────────────────────────────────────────────
% EMBEDDING GEOMETRY, ISOTROPY, AND SPECTRAL ANALYSIS
% ──────────────────────────────────────────────────────────────

@inproceedings{ethayarajh2019contextual,
  title={How Contextual are Contextualized Word Representations? {Comparing} the Geometry of {BERT}, {ELMo}, and {GPT-2} Embeddings},
  author={Ethayarajh, Kawin},
  booktitle={Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing},
  pages={55--65},
  year={2019}
}

@inproceedings{cai2021isotropy,
  title={Isotropy in the Contextual Embedding Space: Clusters and Manifolds},
  author={Cai, Xingyu and Huang, Jiaji and Bian, Yuchen and Church, Kenneth},
  booktitle={International Conference on Learning Representations},
  year={2021}
}

@inproceedings{mu2018allbut,
  title={All-but-the-Top: Simple and Effective Postprocessing for Word Representations},
  author={Mu, Jiaqi and Bhat, Suma and Viswanath, Pramod},
  booktitle={International Conference on Learning Representations},
  year={2018}
}

% ──────────────────────────────────────────────────────────────
% MODELS AND ARCHITECTURES
% ──────────────────────────────────────────────────────────────

@article{radford2019language,
  title={Language Models are Unsupervised Multitask Learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  journal={OpenAI Blog},
  year={2019}
}

@inproceedings{vaswani2017attention,
  title={Attention is All You Need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  booktitle={Advances in Neural Information Processing Systems},
  volume={30},
  year={2017}
}

% ──────────────────────────────────────────────────────────────
% AUTHOR'S CROSS-REFERENCE
% ──────────────────────────────────────────────────────────────

@article{author2026geometry,
  title={Detecting {LLM} Hallucinations via Embedding Cluster Geometry: A Three-Type Taxonomy with Measurable Signatures},
  author={Korun, Matic},
  journal={arXiv preprint arXiv:2602.14259},
  year={2026}
}

@article{author2026induction,
  title={From Prerequisites to Predictions: Testing a Geometric Hallucination Taxonomy Through Controlled Induction in {GPT-2}},
  author={Korun, Matic},
  journal={arXiv preprint arXiv:XXXX.XXXXX},
  year={2026}
}

@article{author2026spectral,
  title={Spectral Signatures of Hallucination Types: Resolving the {Type~1/2} Collapse Through Eigenspectrum Decomposition},
  author={Korun, Matic},
  journal={arXiv preprint arXiv:XXXX.XXXXX},
  year={2026}
}